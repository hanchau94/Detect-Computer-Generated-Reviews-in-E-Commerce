# !pip install transformers

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Set GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set the model name
MODEL_NAME = 'bert-base-uncased'

# Build a BERT based tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Load the basic BERT model
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model = model.to(device)

# load Model 3
model.load_state_dict(torch.load('DL_model_3.pth'))

def predict_labels(model, tokenizer, max_len=160):
    model.eval()
    review = input("Please enter the review you want to check: ")
    
    with torch.no_grad():
        encoding_reviews = tokenizer.encode_plus(
                  review,
                  add_special_tokens=True,
                  max_length= max_len,
                  return_token_type_ids=False,
                  return_attention_mask=True,
                  truncation = True,
                  pad_to_max_length=True,
                  return_tensors='pt',
                  )

        outputs = model(**encoding_reviews.to(device))
        predictions = outputs.logits.argmax(dim=1)


    if predictions ==1:
        return "The review is from Customer"
    else:
        return "This review is generated by Computer"


predict_labels(model, tokenizer)

