# !pip install transformers

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Set GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set the model name
MODEL_NAME = 'bert-base-uncased'

# Build a BERT based tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# load Model 1
model_1 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model_1.to(device)
model_1.load_state_dict(torch.load('DL_model_1.pth'))

# Load Model 3
model_3 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model_3.to(device)
model_3.load_state_dict(torch.load('DL_model_3.pth'))

def predict_labels(model1, model3, tokenizer, max_len=160):
    model1.eval()
    model3.eval()
    review = input("Please enter the review you want to check: ")
    
    with torch.no_grad():
        encoding_reviews = tokenizer.encode_plus(
                  review,
                  add_special_tokens=True,
                  max_length= max_len,
                  return_token_type_ids=False,
                  return_attention_mask=True,
                  truncation = True,
                  pad_to_max_length=True,
                  return_tensors='pt',
                  )

        outputs_1 = model1(**encoding_reviews.to(device))
        outputs_3 = model3(**encoding_reviews.to(device))
        pred_1 = outputs_1.logits.argmax(dim=1)
        pred_3 = outputs_3.logits.argmax(dim=1)
        
        preds = pred_1 + pred_3
        labels = torch.where(preds > 0, torch.tensor(1), torch.tensor(0))

    if labels ==1:
        return "The review is from Customer"
    else:
        return "This review is generated by Computer"

predict_labels(model_1,model_3)
